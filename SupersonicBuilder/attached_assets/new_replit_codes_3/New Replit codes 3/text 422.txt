import os
import json
import platform
import socket
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from flask import Flask, request, render_template_string, jsonify, Response
from dotenv import load_dotenv
load_dotenv()

import sys, time, re
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..")))
from desktop.webui.agent_loader import load_agent

PORT = int(os.getenv("PORT", "7860"))
agent = load_agent()

CACHE_DIR = Path(".supersonic_cache")
CACHE_DIR.mkdir(exist_ok=True)
BENCH_JSON = CACHE_DIR / "bench.json"
BENCH_ONCE = CACHE_DIR / "bench_once.json"

# ---------- tiny utils ----------
def read_env_var(key, default=""):
    val = os.getenv(key)
    if val is not None:
        return val
    envp = Path(".env")
    if envp.exists():
        for line in envp.read_text(encoding="utf-8").splitlines():
            if "=" in line and not line.strip().startswith("#"):
                if ":" in line and "=" not in line:
                    k, v = line.split(":", 1)
                else:
                    k, v = line.split("=", 1)
                if k.strip() == key:
                    return v.strip()
    return default

def detect_status():
    """
    Returns:
      {"status":"ok|cpu|missing|error", "backend":"cuda|rocm|metal|cpu|unknown",
       "model":"name", "details":"..."}
    """
    model_path = read_env_var("AGENT_MODEL", "gguf/llama-3.1-8b-q4_0.gguf")
    backend = "unknown"
    details = ""
    try:
        out = subprocess.check_output([sys.executable, "scripts/gpu_detect.py"], text=True, timeout=6)
        for line in out.splitlines():
            if line.startswith("Backend:"):
                backend = line.split(":", 1)[1].strip()
                break
    except Exception as e:
        details = f"gpu_detect error: {e}"

    model_exists = Path(model_path).exists()
    if not model_exists:
        return {"status": "missing", "backend": backend, "model": Path(model_path).name, "details": "Model file not found"}

    if backend in ("cuda", "rocm", "metal"):
        return {"status": "ok", "backend": backend, "model": Path(model_path).name, "details": details or "accelerated"}
    if backend in ("cpu", "unknown"):
        return {"status": "cpu", "backend": backend, "model": Path(model_path).name, "details": details or "CPU mode"}
    return {"status": "error", "backend": backend, "model": Path(model_path).name, "details": details or "unknown"}

def play_ping(ok: bool, msg: str = ""):
    script = Path("scripts/post_tune_ping.py")
    if not script.exists():
        return {"played": False, "reason": "post_tune_ping.py not found"}
    try:
        if ok:
            subprocess.run([sys.executable, str(script)], check=False)
        else:
            subprocess.run([sys.executable, str(script), "--error", msg or "Triggered error ping"], check=False)
        return {"played": True}
    except Exception as e:
        return {"played": False, "reason": str(e)}

def speak_status_message():
    stat = detect_status()
    ctx = read_env_var("LLM_CTX", "n/a")
    th  = read_env_var("LLM_THREADS", "n/a")
    msg = (
        f"Supersonic status {stat['status']}. "
        f"Backend {stat['backend']}. "
        f"Model {stat['model']}. "
        f"Context {ctx}. Threads {th}."
    )
    script = Path("scripts/post_tune_ping.py")
    if not script.exists():
        return {"spoken": False, "reason": "post_tune_ping.py not found"}
    try:
        subprocess.run([sys.executable, str(script), "--say", msg], check=False)
        return {"spoken": True, "message": msg}
    except Exception as e:
        return {"spoken": False, "reason": str(e)}

def read_bench_cache():
    if BENCH_JSON.exists():
        try:
            return json.loads(BENCH_JSON.read_text(encoding="utf-8"))
        except Exception:
            return {}
    return {}

def save_bench_cache(data: dict):
    try:
        BENCH_JSON.write_text(json.dumps(data, indent=2), encoding="utf-8")
    except Exception:
        pass

def run_bench():
    script = Path("scripts/bench_llm.py")
    if not script.exists():
        return {"ok": False, "error": "bench_llm.py not found"}
    try:
        out = subprocess.check_output([sys.executable, str(script)], text=True, timeout=120)
    except subprocess.TimeoutExpired:
        return {"ok": False, "error": "benchmark timeout"}
    except Exception as e:
        return {"ok": False, "error": str(e)}

    runs = []
    for line in out.splitlines():
        m = re.search(r"Run\s+(\d+):\s*~(\d+)\s+tokens.*?‚Üí\s*([\d\.]+)\s+tok/s", line)
        if m:
            runs.append({"run": int(m.group(1)), "gen_tokens": int(m.group(2)), "toks_per_sec": float(m.group(3))})
    avg = None
    m2 = re.search(r"Avg:\s*~(\d+)\s+tokens.*?([\d\.]+)\s+tok/s", out)
    if m2:
        avg = {"avg_tokens_per_run": int(m2.group(1)), "avg_tok_per_sec": float(m2.group(2))}
    res = {"ok": True, "runs": runs, "avg": avg, "raw": out, "ts": int(time.time())}
    save_bench_cache(res)
    return res

def reset_bench_cache():
    deleted = []
    for p in (BENCH_JSON, BENCH_ONCE):
        try:
            if p.exists():
                p.unlink()
                deleted.append(p.name)
        except Exception:
            pass
    return deleted

def git_commit_short():
    try:
        out = subprocess.check_output(["git", "rev-parse", "--short", "HEAD"], text=True, stderr=subprocess.DEVNULL, timeout=3)
        return out.strip()
    except Exception:
        return None

def make_export_payload():
    bench = read_bench_cache()
    if not bench or not isinstance(bench, dict):
        bench = run_bench()
    stat = detect_status()
    meta = {
        "hostname": socket.gethostname(),
        "os": f"{platform.system()} {platform.release()} {platform.machine()}",
        "python": platform.python_version(),
        "backend": stat.get("backend", "unknown"),
        "model": stat.get("model", "n/a"),
        "commit": git_commit_short() or "n/a",
        "utc": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")
    }
    return {"meta": meta, "benchmark": bench}

def make_env_summary():
    keys = ["AGENT_MODEL", "LLM_CTX", "LLM_THREADS", "DOCS_DIR", "PORT"]
    data = {k: read_env_var(k, "") for k in keys}
    try:
        if data.get("AGENT_MODEL"):
            data["AGENT_MODEL_NAME"] = Path(data["AGENT_MODEL"]).name
    except Exception:
        pass
    stat = detect_status()
    data["BACKEND"] = stat.get("backend", "unknown")
    data["MODEL_PRESENT"] = "yes" if stat.get("status") in ("ok", "cpu") else "no"
    return data

# ---- Model autoscan helpers ----
def _score_model_path(p: Path) -> float:
    try:
        stat = p.stat()
        size = stat.st_size
        mtime = stat.st_mtime
    except Exception:
        return 0.0
    name = p.name.lower()
    boost = 0.0
    for kw, b in [("llama", 5.0), ("mistral", 4.5), ("qwen", 4.0), ("gemma", 3.8), ("phi", 3.5)]:
        if kw in name:
            boost = max(boost, b)
    if "q4" in name:
        boost += 1.0
    elif "q5" in name or "q6" in name:
        boost += 0.8
    elif "q2" in name:
        boost -= 0.5
    return (size / 1e6) * 1.0 + (mtime / 1e5) * 0.02 + boost

def _find_gguf_candidates() -> list:
    roots = []
    cur = read_env_var("AGENT_MODEL", "").strip()
    if cur:
        try:
            roots.append(Path(cur).expanduser().resolve().parent)
        except Exception:
            pass
    for r in ["gguf", "models", "weights", "."]:
        roots.append(Path(r).resolve())

    seen = set()
    found = []
    for root in roots:
        try:
            if not root.exists():
                continue
            for p in root.rglob("*.gguf"):
                rp = p.resolve()
                if rp in seen:
                    continue
                seen.add(rp)
                score = _score_model_path(rp)
                try:
                    st = rp.stat()
                    found.append({
                        "path": str(rp),
                        "size": st.st_size,
                        "mtime": st.st_mtime,
                        "score": score
                    })
                except Exception:
                    continue
        except Exception:
            continue
    found.sort(key=lambda x: x["score"], reverse=True)
    return found

def _suggest_model_path() -> str:
    cur = read_env_var("AGENT_MODEL", "").strip()
    if cur:
        p = Path(cur).expanduser()
        if p.exists():
            return str(p)
    cands = _find_gguf_candidates()
    if cands:
        return cands[0]["path"]
    return "gguf/llama-3.1-8b-q4_0.gguf"

# ---------- HTML ----------
TPL = '''
<!doctype html>
<html>
<head>
  <meta charset="utf-8"/>
  <title>Supersonic Local</title>
  <style>
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; margin:20px; }
    .bar { display:flex; align-items:center; gap:12px; margin-bottom:14px; flex-wrap:wrap; }
    .led { width:12px; height:12px; border-radius:50%; box-shadow:0 0 6px rgba(0,0,0,.25); }
    .led.ok { background:#10b981; }
    .led.cpu { background:#f59e0b; }
    .led.missing, .led.error { background:#ef4444; }
    .led.ready-ok { background:#10b981; }
    .led.ready-no { background:#ef4444; }
    .tag { padding:2px 8px; border-radius:999px; font-size:12px; background:#111; color:#fff; }
    .muted { color:#888; font-size:12px; }
    .row { display:flex; gap:10px; align-items:center; flex-wrap:wrap; }
    input[name="q"] { width:70%; padding:8px; font-size:14px; }
    button, a.pill { padding:8px 12px; font-size:14px; }
    pre { background:#0b0b0b; color:#e5e7eb; padding:12px; border-radius:8px; white-space:pre-wrap; }
    .pill { border:1px solid #333; background:#171717; color:#e5e7eb; text-decoration:none; display:inline-block; border-radius:8px; cursor:pointer; }
    .card { border:1px solid #2a2a2a; background:#0f0f10; color:#eee; border-radius:10px; padding:12px; margin-top:14px; }
    .card h3 { margin:0 0 8px 0; font-size:16px; }
    .kv { display:flex; gap:12px; flex-wrap:wrap; font-size:13px; color:#bbb; }
    .kv span { background:#151515; padding:2px 8px; border-radius:6px; }
    .links { display:flex; gap:8px; flex-wrap:wrap; margin-top:8px; }
    .help { border:1px solid #333; background:#111; color:#fff; border-radius:12px; padding:0 8px; line-height:22px; height:22px; cursor:pointer; }
    .toast { position:fixed; right:16px; bottom:16px; background:#111; color:#fff; border:1px solid #333; border-radius:8px; padding:10px 12px; opacity:0; transform:translateY(8px); transition:all .2s ease; }
    .toast.show { opacity:1; transform:translateY(0); }
    .modal { position:fixed; inset:0; background:rgba(0,0,0,.55); display:none; align-items:center; justify-content:center; z-index:9999; }
    .modal.show { display:flex; }
    .modal-content { width:min(780px,92vw); background:#0f0f10; color:#eee; border:1px solid #2a2a2a; border-radius:12px; padding:16px; box-shadow:0 10px 30px rgba(0,0,0,.5); }
    .modal-content h3 { margin:0 0 8px 0; }
    .modal-content pre { background:#0b0b0b; max-height:40vh; overflow:auto; }
    .modal-actions { display:flex; gap:8px; justify-content:flex-end; margin-top:10px; }
  </style>
</head>
<body>
  <div class="bar">
    <!-- STATUS LED -->
    <div id="led" class="led cpu" title="Status LED"></div>
    <div id="statusText" class="muted" title="Gathering status‚Ä¶">Status: checking‚Ä¶</div>
    <span id="backend" class="tag" title="backend engine">backend: ‚Ä¶</span>
    <span id="model" class="tag" title="model file">model: ‚Ä¶</span>
    <button class="help" onclick="openStatusHelp()">?</button>

    <!-- READY LED -->
    <div id="ledReady" class="led ready-no" title="Readiness LED" style="margin-left:16px;"></div>
    <div id="readyText" class="muted" title="Evaluating readiness‚Ä¶">Ready: checking‚Ä¶</div>
    <button class="help" onclick="openReadyHelp()">?</button>

    <button class="pill" onclick="pingSound(true)"  title="Play success chime">Test ‚úÖ</button>
    <button class="pill" onclick="pingSound(false)" title="Play error chime">Test ‚ùå</button>
    <button class="pill" onclick="speakStatus()" title="Speak current status">Speak üó£Ô∏è</button>
  </div>

  <div class="row">
    <form method="post" action="/ask">
      <input name="q" placeholder="Ask the agent‚Ä¶"/>
      <button class="pill">Ask</button>
    </form>
    <form action="/api/ingest" method="post" style="margin-left:4px;">
      <button class="pill" title="Ingest docs from DOCS_DIR">Ingest Docs</button>
    </form>
  </div>

  <div class="card" id="benchCard">
    <h3>Performance</h3>
    <div class="kv">
      <span id="benchAvg">avg: ‚Ä¶ tok/s</span>
      <span id="benchTokens">tokens/run: ‚Ä¶</span>
      <span id="benchWhen">updated: ‚Ä¶</span>
    </div>
    <div class="links">
      <button class="pill" onclick="runBench()">Run Bench</button>
      <button class="pill" onclick="resetBench()">Reset & Re-Bench</button>
      <button class="pill" onclick="pingAll()">Ping All üöÄ</button>
      <a class="pill" href="/api/bench/export" title="Download latest bench JSON" target="_blank">Download Bench JSON ‚¨áÔ∏è</a>
    </div>
  </div>

  <div class="card" id="apiQuick">
    <h3>API Quick Actions (copy curl)</h3>
    <div style="display:flex; flex-wrap:wrap; gap:8px;">
      <button class="pill" onclick="copyCurl('bench')">curl POST /api/bench</button>
      <button class="pill" onclick="copyCurl('reset')">curl POST /api/bench/reset</button>
      <button class="pill" onclick="copyCurl('ok')">curl POST /api/status/sound?ok=1</button>
      <button class="pill" onclick="copyCurl('err')">curl POST /api/status/sound?ok=0&msg=Model%20missing</button>
      <button class="pill" onclick="copyCurl('speak')">curl POST /api/status/speak</button>
      <button class="pill" onclick="copyCurl('export')">curl GET /api/bench/export</button>
      <button class="pill" onclick="copyCurl('pingall')">curl POST /api/pingall</button>
    </div>
    <div class="links">
      <a class="pill" href="/api/status" target="_blank">Open /api/status</a>
      <a class="pill" href="/api/readyz" target="_blank">Open /api/readyz</a>
      <a class="pill" href="/api/bench" target="_blank">Open /api/bench (GET)</a>
      <a class="pill" href="/api/bench/export" target="_blank">Open /api/bench/export</a>
    </div>
  </div>

  <div class="card" id="envCard">
    <h3>Environment</h3>
    <div class="kv" id="envKv"></div>
    <div class="links">
      <button class="pill" onclick="copyEnv()">Copy .env summary</button>
      <button class="pill" onclick="runFullCheck()">Run Full System Check</button>
      <a class="pill" href="/api/env/scan" target="_blank">Scan models</a>
      <a class="pill" href="/api/env/summary" target="_blank">View .env summary</a>
    </div>
  </div>

  <pre id="resp">{{ resp }}</pre>

  <!-- Modal -->
  <div id="modal" class="modal" onclick="modalBgClose(event)">
    <div class="modal-content">
      <h3 id="modalTitle">Help</h3>
      <div id="modalBody" style="font-size:14px; line-height:1.45;"></div>
      <pre id="modalJson"></pre>
      <div class="modal-actions">
        <button class="pill" onclick="fixReady()">Fix it for me</button>
        <button class="pill" onclick="copyModal()">Copy details</button>
        <button class="pill" onclick="closeModal()">Close</button>
      </div>
    </div>
  </div>

  <div id="toast" class="toast">Copied!</div>

<script>
function tsToLocal(ts){
  if(!ts) return 'n/a';
  try{ return new Date(ts*1000).toLocaleString(); }catch(e){ return ''+ts; }
}

let _lastStatus = null;
let _lastReady  = null;

async function refreshStatus(){
  try{
    const r = await fetch('/api/status');
    const j = await r.json();
    _lastStatus = j;
    const led = document.getElementById('led');
    const st  = document.getElementById('statusText');
    const be  = document.getElementById('backend');
    const md  = document.getElementById('model');

    const details = (j.details || '').trim();
    led.className = 'led ' + j.status;

    const statusTip = [
      `status=${j.status}`,
      j.backend ? `backend=${j.backend}` : null,
      j.model   ? `model=${j.model}`     : null,
      details   ? `details=${details}`   : null
    ].filter(Boolean).join(' ¬∑ ');

    led.title = statusTip || 'Status LED';
    st.title  = statusTip || 'Status';
    be.title  = `backend=${j.backend || 'unknown'}`;
    md.title  = `model=${j.model || 'n/a'}`;

    st.textContent = `Status: ${j.status}${details ? ' ‚Äî ' + details : ''}`;
    be.textContent = `backend: ${j.backend || 'unknown'}`;
    md.textContent = `model: ${j.model || 'n/a'}`;
  }catch(e){
    const led = document.getElementById('led');
    const st  = document.getElementById('statusText');
    led.className = 'led error';
    led.title = 'Status: unreachable';
    st.textContent = 'Status: error ‚Äî cannot reach /api/status';
    st.title = 'Cannot reach /api/status';
  }
}

async function refreshReady(){
  try{
    const r = await fetch('/api/readyz');
    const j = await r.json();
    _lastReady = j;
    const ledR = document.getElementById('ledReady');
    const rt  = document.getElementById('readyText');

    ledR.className = 'led ' + (j.ok ? 'ready-ok' : 'ready-no');

    const parts = [
      `ok=${j.ok ? 'true' : 'false'}`,
      j.backend         ? `backend=${j.backend}` : null,
      `model_ready=${j.model_ready ? 'true' : 'false'}`,
      `backend_known=${j.backend_known ? 'true' : 'false'}`
    ];
    if (j.require_recent_bench !== undefined) {
      parts.push(`require_recent_bench=${j.require_recent_bench ? 'true' : 'false'}`);
      if (j.require_recent_bench) {
        parts.push(`bench_ok=${j.bench_ok ? 'true' : 'false'}`);
        if (j.bench_age_sec !== null && j.bench_age_sec !== undefined) {
          parts.push(`bench_age_sec=${j.bench_age_sec}`);
        }
        parts.push(`bench_fresh_sec=${j.bench_fresh_sec}`);
      }
    }
    const tip = parts.filter(Boolean).join(' ¬∑ ');
    ledR.title = tip || 'Readiness LED';
    const extra = (j.require_recent_bench && !j.bench_ok) ? ' (bench stale)' : '';
    rt.textContent = `Ready: ${j.ok ? 'yes' : 'no'}${j.backend ? ' backend=' + j.backend : ''}${extra}`;
    rt.title = tip || 'Readiness details';
  }catch(e){
    const ledR = document.getElementById('ledReady');
    const rt  = document.getElementById('readyText');
    ledR.className = 'led ready-no';
    ledR.title = 'Readiness: unreachable';
    rt.textContent = 'Ready: error ‚Äî cannot reach /api/readyz';
    rt.title = 'Cannot reach /api/readyz';
  }
}

async function loadBench(){
  try{
    const r = await fetch('/api/bench', {method:'GET'});
    const j = await r.json();
    const a = document.getElementById('benchAvg');
    const t = document.getElementById('benchTokens');
    const w = document.getElementById('benchWhen');

    if(j.ok && j.avg){
      a.textContent = `avg: ${j.avg.avg_tok_per_sec.toFixed(1)} tok/s`;
      t.textContent = `tokens/run: ${j.avg.avg_tokens_per_run}`;
      w.textContent = `updated: ${tsToLocal(j.ts)}`;
    }else if(j.ok){
      a.textContent = `avg: n/a`;
      t.textContent = `tokens/run: n/a`;
      w.textContent = `updated: ${tsToLocal(j.ts)}`;
    }else{
      a.textContent = `avg: n/a`;
      t.textContent = `tokens/run: n/a`;
      w.textContent = `updated: error`;
    }
  }catch(e){
    console.warn('bench load error', e);
  }
}

async function runBench(){
  try{
    await fetch('/api/bench', {method:'POST'});
    await loadBench();
    await refreshReady();
  }catch(e){
    console.warn('bench error', e);
  }
}

async function resetBench(){
  try{
    await fetch('/api/bench/reset', {method:'POST'});
    await loadBench();
    await refreshReady();
  }catch(e){
    console.warn('bench reset error', e);
  }
}

async function pingAll(){
  try{
    const r = await fetch('/api/pingall', {method:'POST'});
    const j = await r.json();
    await loadBench();
    await refreshReady();
    showToast(j.ok ? 'Ping All OK' : ('Ping All: ' + (j.reason || 'error')));
  }catch(e){
    console.warn('pingall error', e);
    showToast('Ping All error');
  }
}

function showToast(text){
  const t = document.getElementById('toast');
  t.textContent = text || 'Copied!';
  t.classList.add('show');
  setTimeout(()=> t.classList.remove('show'), 1400);
}

async function copyCurl(kind){
  const base = window.location.origin;
  let cmd = '';
  switch(kind){
    case 'bench':   cmd = `curl -X POST "${base}/api/bench"`; break;
    case 'reset':   cmd = `curl -X POST "${base}/api/bench/reset"`; break;
    case 'ok':      cmd = `curl -X POST "${base}/api/status/sound?ok=1"`; break;
    case 'err':     cmd = `curl -X POST "${base}/api/status/sound?ok=0&msg=Model%20missing"`; break;
    case 'speak':   cmd = `curl -X POST "${base}/api/status/speak"`; break;
    case 'export':  cmd = `curl "${base}/api/bench/export" -o bench_export.json`; break;
    case 'pingall': cmd = `curl -X POST "${base}/api/pingall"`; break;
    default:        cmd = '# unknown'; break;
  }
  try{
    await navigator.clipboard.writeText(cmd);
    showToast('Copied curl ‚Üí clipboard');
  }catch(e){
    const ta = document.createElement('textarea');
    ta.value = cmd;
    document.body.appendChild(ta);
    ta.select();
    document.execCommand('copy');
    document.body.removeChild(ta);
    showToast('Copied (fallback)');
  }
}

async function copyEnv(){
  try{
    const r = await fetch('/api/env/summary');
    const j = await r.json();
    const text = JSON.stringify(j, null, 2);
    await navigator.clipboard.writeText(text);
    showToast('.env summary copied');
  }catch(e){
    console.warn('env copy error', e);
  }
}

/* ---------- Help modal plumbing ---------- */
function openModal(title, bodyHtml, jsonObj){
  const m = document.getElementById('modal');
  document.getElementById('modalTitle').textContent = title || 'Help';
  document.getElementById('modalBody').innerHTML = bodyHtml || '';
  const pj = document.getElementById('modalJson');
  pj.textContent = jsonObj ? JSON.stringify(jsonObj, null, 2) : '';
  m.classList.add('show');
}
function closeModal(){ document.getElementById('modal').classList.remove('show'); }
function modalBgClose(ev){ if(ev.target.id === 'modal'){ closeModal(); } }
document.addEventListener('keydown', (e)=>{ if(e.key === 'Escape'){ closeModal(); }});
async function copyModal(){
  const pj = document.getElementById('modalJson').textContent || '';
  try{
    await navigator.clipboard.writeText(pj);
    showToast('Details copied');
  }catch(e){ showToast('Copy failed'); }
}

function openStatusHelp(){
  const j = _lastStatus || {};
  const fixes = [
    '<b>missing</b>: Place the model file at <code>$AGENT_MODEL</code> or set AGENT_MODEL in <code>.env</code> to an existing path.',
    '<b>cpu</b>: GPU not detected. If you expect GPU, check drivers (CUDA/ROCm/Metal), then re-run <code>scripts/gpu_detect.py</code>.',
    '<b>error/unknown</b>: See <i>details</i>. Often a permission/path issue.'
  ].join('<br>');
  const body = `
    <p><b>Status LED</b> reflects <code>/api/status</code>.</p>
    <ul>
      <li><b>status</b>: ok | cpu | missing | error</li>
      <li><b>backend</b>: cuda | rocm | metal | cpu | unknown</li>
      <li><b>model</b>: model filename</li>
      <li><b>details</b>: extra info from detector</li>
    </ul>
    <p><b>Fixes</b><br>${fixes}</p>
  `;
  openModal('Status LED Help', body, j);
}

function openReadyHelp(){
  const j = _lastReady || {};
  const body = `
    <p><b>Ready LED</b> reflects <code>/api/readyz</code>.</p>
    <ul>
      <li><b>ok</b>: all readiness checks passed</li>
      <li><b>model_ready</b>: model present (status ok|cpu)</li>
      <li><b>backend_known</b>: backend recognized (cuda|rocm|metal|cpu)</li>
      <li><b>require_recent_bench</b>: if true, bench must be fresh</li>
      <li><b>bench_ok</b>: bench within freshness window</li>
      <li><b>bench_age_sec</b> vs <b>bench_fresh_sec</b></li>
    </ul>
    <p><b>Common fixes</b></p>
    <ol>
      <li>If <code>model_ready=false</code>: set <code>AGENT_MODEL</code> in <code>.env</code> or place the .gguf file.</li>
      <li>If <code>backend_known=false</code>: install/enable GPU runtime (CUDA/ROCm/Metal) or continue in CPU.</li>
      <li>If bench is stale and required: click <b>Run Bench</b> or disable freshness via <code>REQUIRE_RECENT_BENCH=0</code>.</li>
    </ol>
  `;
  openModal('Ready LED Help', body, j);
}

/* ---------- Fix it for me ---------- */
async function fixReady(){
  const did = [];
  try{
    const rzResp = await fetch('/api/readyz'); const rz = await rzResp.json();
    const envp   = await (await fetch('/api/env/present')).json();

    if (rz.require_recent_bench && !rz.bench_ok) {
      await fetch('/api/bench', { method: 'POST' });
      did.push('ran benchmark');
    }

    if (envp && envp.present === false) {
      const t = await fetch('/api/env/template');
      const content = await t.text();
      const w = await fetch('/api/env/write', {
        method: 'POST',
        headers: {'Content-Type': 'text/plain'},
        body: content
      });
      const wj = await w.json();
      if (wj.ok) {
        did.push('wrote .env from template');
        await fetch('/api/reload', { method: 'POST' });
        return; // process will exit; runner should restart
      } else {
        console.warn('env write failed', wj);
        did.push('FAILED to write .env (check permissions)');
      }
    } else {
      if (!rz.model_ready) {
        window.open('/api/env/template', '_blank');
        did.push('opened .env template (model not ready)');
      }
    }

    await loadBench();
    await refreshReady();
    showToast(did.length ? `Fixed: ${did.join(', ')}` : 'Nothing to fix right now');
  } catch (e) {
    console.warn('fixReady error', e);
    showToast('Fix failed');
  }
}

/* ---------- System check ---------- */
async function runFullCheck(){
  try{
    const r = await fetch('/api/env/test');
    const j = await r.json();
    const ok = !!j.ok;
    const head = `<p><b>Result:</b> ${ok ? '‚úÖ OK' : '‚ùå Issues detected'}</p>`;
    const list = (arr, title) => (arr && arr.length)
      ? `<p><b>${title}</b><br><ul>` + arr.map(x=>`<li>${x}</li>`).join('') + `</ul></p>` : '';
    const bench = j.bench || {};
    const benchRow = `
      <p><b>Bench</b><br>
      required=${bench.required ? 'true':'false'} ¬∑
      have_bench=${bench.have_bench ? 'true':'false'} ¬∑
      bench_ok=${bench.bench_ok ? 'true':'false'} ¬∑
      fresh_sec=${bench.fresh_sec}${bench.bench_age_sec!=null ? ' ¬∑ bench_age_sec='+bench.bench_age_sec : ''}</p>
    `;
    const body = `
      ${head}
      <p><b>Backend:</b> ${j.backend || 'unknown'}</p>
      ${benchRow}
      ${list(j.issues, 'Issues')}
      ${list(j.suggestions, 'Suggestions')}
      ${list(j.notes, 'Notes')}
    `;
    openModal('Full System Check', body, j);
  }catch(e){
    console.warn('runFullCheck error', e);
    showToast('Full System Check failed');
  }
}

/* ---------- tiny actions ---------- */
async function pingSound(ok){
  const url = ok ? '/api/status/sound?ok=1' : '/api/status/sound?ok=0&msg=Manual%20error%20test';
  try{ await fetch(url, {method:'POST'}); }catch(e){ console.warn(e); }
}
async function speakStatus(){
  try{ await fetch('/api/status/speak', {method:'POST'}); }catch(e){ console.warn(e); }
}

refreshStatus();
refreshReady();
setInterval(refreshStatus, 3000);
setInterval(refreshReady, 3500);
loadBench();
</script>
</body>
</html>
'''

app = Flask(__name__)

@app.route("/", methods=["GET"])
def index():
    return render_template_string(TPL, resp="")

@app.route("/ask", methods=["POST"])
def ask():
    q = request.form.get("q","").strip()
    out = agent.query(q)
    return render_template_string(TPL, resp=out)

@app.route("/api/ingest", methods=["POST"])
def ingest():
    status = "ingested"
    try:
        d = Path(os.getenv("DOCS_DIR","docs"))
        status = agent.ingest_dir(d)
    except Exception as e:
        status = f"error: {e}"
    return jsonify({"status": status})

@app.route("/api/status", methods=["GET"])
def api_status():
    return jsonify(detect_status())

@app.route("/api/status/sound", methods=["POST"])
def api_status_sound():
    ok = request.args.get("ok", "1") in ("1","true","True","yes","y")
    msg = request.args.get("msg", "")
    res = play_ping(ok, msg)
    return jsonify(res)

@app.route("/api/status/speak", methods=["POST"])
def api_status_speak():
    res = speak_status_message()
    return jsonify(res)

@app.route("/api/bench", methods=["GET","POST"])
def api_bench():
    if request.method == "GET":
        cache = read_bench_cache()
        if cache:
            return jsonify({**cache, "ok": True})
        return jsonify({"ok": True, "avg": None, "ts": None})
    res = run_bench()
    return jsonify(res)

@app.route("/api/bench/reset", methods=["POST"])
def api_bench_reset():
    deleted = reset_bench_cache()
    res = run_bench()
    return jsonify({"deleted": deleted, **res})

@app.route("/api/bench/export", methods=["GET"])
def api_bench_export():
    payload = make_export_payload()
    body = json.dumps(payload, indent=2)
    headers = {"Content-Disposition": "attachment; filename=bench_export.json"}
    return Response(body, mimetype="application/json", headers=headers)

@app.route("/api/env/summary", methods=["GET"])
def api_env_summary():
    return jsonify(make_env_summary())

@app.route("/api/pingall", methods=["POST"])
def api_ping_all():
    stat = detect_status()
    ok = stat.get("status") in ("ok", "cpu")
    reason = None
    if not ok:
        reason = f"status={stat.get('status')} details={stat.get('details','')}"
    bench = run_bench() if ok else {"ok": False, "error": "status not ready"}
    speak = speak_status_message()
    if ok:
        play_ping(True, "")
    else:
        play_ping(False, reason or "not ready")
    return jsonify({
        "ok": ok and bool(bench.get("ok")),
        "status": stat,
        "bench_ok": bench.get("ok", False),
        "avg": (bench.get("avg") or {}).get("avg_tok_per_sec"),
        "spoken": speak.get("spoken", False),
        "reason": reason
    })

# --- Health endpoints (liveness / readiness) ---
@app.route("/api/healthz", methods=["GET", "HEAD"])
@app.route("/api/livez",  methods=["GET", "HEAD"])
def api_healthz():
    return jsonify({"ok": True, "ts": int(time.time())})

@app.route("/api/readyz", methods=["GET"])
def api_readyz():
    """
    Readiness: require model present and backend known (ok/cpu/metal/cuda/rocm).
    Optionally require a recent bench result (‚â§ BENCH_FRESH_SEC) if REQUIRE_RECENT_BENCH=1.
    """
    REQUIRE_RECENT_BENCH = os.getenv("REQUIRE_RECENT_BENCH", "0") in ("1","true","True","yes","y")
    BENCH_FRESH_SEC = int(os.getenv("BENCH_FRESH_SEC", "43200"))  # 12h

    stat = detect_status()
    backend = stat.get("backend")
    status  = stat.get("status")

    model_ready   = status in ("ok", "cpu")
    backend_known = backend in ("cuda", "rocm", "metal", "cpu")

    bench_ok = True
    bench_age = None
    if REQUIRE_RECENT_BENCH:
        cache = read_bench_cache()
        if cache and isinstance(cache, dict) and cache.get("ts"):
            bench_age = int(time.time()) - int(cache["ts"])
            bench_ok = bench_age <= BENCH_FRESH_SEC
        else:
            bench_ok = False

    ok = bool(model_ready and backend_known and bench_ok)
    return jsonify({
        "ok": ok,
        "model_ready": model_ready,
        "backend_known": backend_known,
        "backend": backend,
        "status": status,
        "require_recent_bench": REQUIRE_RECENT_BENCH,
        "bench_fresh_sec": BENCH_FRESH_SEC,
        "bench_ok": bench_ok,
        "bench_age_sec": bench_age
    })

# ---- .env helpers ----
def _env_present() -> bool:
    return Path(".env").exists()

@app.route("/api/env/present", methods=["GET"])
def api_env_present():
    return jsonify({"ok": True, "present": _env_present()})

@app.route("/api/env/template", methods=["GET"])
def api_env_template():
    suggested_model = _suggest_model_path()
    sample = [
        f'AGENT_MODEL={suggested_model}',
        f'LLM_CTX={read_env_var("LLM_CTX","4096")}',
        f'LLM_THREADS={read_env_var("LLM_THREADS","4")}',
        f'DOCS_DIR={read_env_var("DOCS_DIR","docs")}',
        f'PORT={PORT}',
        '# REQUIRE_RECENT_BENCH=1  # uncomment to enforce fresh bench for readiness',
        '# BENCH_FRESH_SEC=43200   # 12h freshness window'
    ]
    body = "\n".join(sample) + "\n"
    return Response(body, mimetype="text/plain",
                    headers={"Content-Disposition": "inline; filename=.env.template"})

@app.route("/api/env/write", methods=["POST"])
def api_env_write():
    content = request.get_data(as_text=True) or request.form.get("content", "")
    if not content.strip():
        return jsonify({"ok": False, "error": "no content provided"})
    try:
        Path(".env").write_text(content, encoding="utf-8")
        return jsonify({"ok": True})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)})

@app.route("/api/env/scan", methods=["GET"])
def api_env_scan():
    try:
        cands = _find_gguf_candidates()
        return jsonify({"ok": True, "candidates": cands})
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)})

# ---- safer reload endpoint (process will exit; runner should restart it) ----
@app.route("/api/reload", methods=["POST"])
def api_reload():
    try:
        # Add a header token check here if you want to restrict callers
        sys.stdout.flush()
        sys.stderr.flush()
        os._exit(0)
    except Exception as e:
        return jsonify({"ok": False, "error": str(e)}), 500

# ---- .env self-check ----
@app.route("/api/env/test", methods=["GET"])
def api_env_test():
    issues, suggestions, notes = [], [], []
    envp = Path(".env")
    if not envp.exists():
        issues.append(".env is missing")
        suggestions.append("Open /api/env/template and save it as .env (or use Fix it for me).")

    model = read_env_var("AGENT_MODEL", "").strip()
    docs  = read_env_var("DOCS_DIR", "docs").strip()
    port  = read_env_var("PORT", str(PORT)).strip()
    th    = read_env_var("LLM_THREADS", "4").strip()
    ctx   = read_env_var("LLM_CTX", "4096").strip()

    if not model:
        issues.append("AGENT_MODEL not set")
        suggestions.append("Use autoscan template; it will fill the best local .gguf.")
    else:
        mp = Path(model).expanduser()
        if not mp.exists() or not mp.is_file():
            issues.append(f"AGENT_MODEL path not found: {mp}")
            suggestions.append(f"Consider: {_suggest_model_path()}")

    dp = Path(docs)
    if not dp.exists():
        notes.append(f"DOCS_DIR does not exist yet: {dp} (will be created on first ingest)")
        suggestions.append(f"Create directory: {dp}")

    try:
        pv = int(port)
        if pv < 1024 or pv > 65535:
            issues.append(f"PORT out of range: {pv}")
            suggestions.append("Pick a port between 1024‚Äì65535 (e.g., 7860).")
    except Exception:
        issues.append(f"PORT not an integer: {port}")
        suggestions.append("Set PORT to an integer (e.g., 7860).")

    try:
        tv = int(th)
        if tv < 1:
            issues.append(f"LLM_THREADS < 1: {tv}")
            suggestions.append("Set LLM_THREADS to a positive integer.")
    except Exception:
        issues.append(f"LLM_THREADS not an integer: {th}")
        suggestions.append("Set LLM_THREADS to a positive integer (e.g., 4 or number of cores).")

    try:
        cv = int(ctx)
        if cv < 1024:
            notes.append(f"LLM_CTX is small ({cv}); consider 4096+ unless memory-constrained.")
    except Exception:
        notes.append(f"LLM_CTX not an integer: {ctx}; defaulting internally.")

    stat = detect_status()
    backend = stat.get("backend")
    status  = stat.get("status")
    if status not in ("ok", "cpu"):
        issues.append(f"Runtime not ready: status={status}")
        if status == "missing":
            suggestions.append("Place the model file or set AGENT_MODEL via .env template.")
        else:
            suggestions.append("Check GPU drivers or continue in CPU mode; rerun gpu_detect.py.")

    if backend not in ("cuda", "rocm", "metal", "cpu"):
        notes.append(f"Backend unknown: {backend}. If expecting GPU, check drivers.")

    REQUIRE_RECENT_BENCH = os.getenv("REQUIRE_RECENT_BENCH", "0") in ("1","true","True","yes","y")
    bench_info = read_bench_cache()
    bench_ok = True
    bench_age = None
    BENCH_FRESH_SEC = int(os.getenv("BENCH_FRESH_SEC", "43200"))
    if REQUIRE_RECENT_BENCH:
        if bench_info and bench_info.get("ts"):
            bench_age = int(time.time()) - int(bench_info["ts"])
            bench_ok = bench_age <= BENCH_FRESH_SEC
            if not bench_ok:
                issues.append("Benchmark is stale for readiness policy.")
                suggestions.append("Click Run Bench or use Fix it for me.")
        else:
            issues.append("No benchmark found but freshness required.")
            suggestions.append("Click Run Bench or disable REQUIRE_RECENT_BENCH.")

    return jsonify({
        "ok": len(issues) == 0,
        "env_present": envp.exists(),
        "status": stat,
        "backend": backend,
        "issues": issues,
        "suggestions": suggestions,
        "notes": notes,
        "bench": {
            "required": REQUIRE_RECENT_BENCH,
            "fresh_sec": BENCH_FRESH_SEC,
            "have_bench": bool(bench_info),
            "bench_ts": bench_info.get("ts") if isinstance(bench_info, dict) else None,
            "bench_age_sec": bench_age,
            "bench_ok": bench_ok
        }
    })

if __name__ == "__main__":
    app.run(host="127.0.0.1", port=PORT, debug=False)